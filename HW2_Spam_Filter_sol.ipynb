{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW02-Spam Filter_sol",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wpwo98/IDS-CB35533/blob/main/HW02_Spam_Filter_sol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zik1Z3gHtjYW"
      },
      "source": [
        "# HW02. Simple Naive Bayes Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggYBPhcJuuAu"
      },
      "source": [
        "## T1. Load a dataset\n",
        "\n",
        "The following code loads a dataset consisting of text messages and spam-ham labels.\n",
        "\n",
        "You can write your own code below the **\"TODOs\"** to answer the questions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UUgT0MW8Rmj"
      },
      "source": [
        "### Questions:\n",
        "* Number of spam messges? [*747*]\n",
        "* Number of ham messages? [*4825*]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2napWf8tINR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c2aef1-b494-4b6b-bdac-bf225f0f8c36"
      },
      "source": [
        "from typing import List, Tuple, Dict, Iterable, Set\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/mlee-pnu/IDS/main/spam_dataset.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# TODOs\n",
        "hams = df['Category'].value_counts()[\"ham\"]\n",
        "spams = df['Category'].value_counts()[\"spam\"]\n",
        "print(spams, hams)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "747 4825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5wryED8lOBP"
      },
      "source": [
        "## T2. Spam filter for individual words\n",
        "\n",
        "We first defined a function ***tokenize()*** to convert a given text into a set of words. \n",
        "\n",
        "Using the function, we now try to count the frequency of each word in each class (spam and ham).\n",
        "\n",
        "Complete the following code and answer the following questions:\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9VmU28wPHcH"
      },
      "source": [
        "### Qeustions: \n",
        "*   Number of spam messages containing the word \"free\": [*170*]\n",
        "\n",
        "Let's assume we only care for the word \"free\" to determine if a message is a spam or not. Answer the following questions:\n",
        "\n",
        "*   P ( *ham* | *free* ) = [*0.26*]\n",
        "*   Is this message spam? [*Yes*]\n",
        "\n",
        "*Note: Do not apply a smoothing method here.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6rmDIBJnFNh"
      },
      "source": [
        "def tokenize(text: str) -> Set[str]:\n",
        "    text = text.lower()                         \n",
        "    all_words = re.findall(\"[a-z0-9']+\", text)  \n",
        "    return set(all_words)                       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GtcK6qXlrES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e1ad0ec-a9f5-4f37-87d7-05d4bd08e618"
      },
      "source": [
        "tokens: Set[str] = set()\n",
        "token_spam_counts: Dict[str, int] = defaultdict(int)\n",
        "token_ham_counts: Dict[str, int] = defaultdict(int)\n",
        "\n",
        "spam = df[df.Category == 'spam']\n",
        "ham = df[df.Category == 'ham']\n",
        "\n",
        "for msg in spam['Message'].to_list():\n",
        "  for token in tokenize(msg):\n",
        "    tokens.add(token)\n",
        "    token_spam_counts[token] += 1\n",
        "\n",
        "for msg in ham['Message'].to_list():\n",
        "  for token in tokenize(msg):\n",
        "    tokens.add(token)\n",
        "    token_ham_counts[token] += 1\n",
        "\n",
        "# TODOs\n",
        "word = \"free\"\n",
        "n_word_spam = token_spam_counts[word] # frequency of the word in spam messages\n",
        "n_word_ham = token_ham_counts[word]   # frequency of the word in ham messages\n",
        "print(\"counts in spam:\",n_word_spam, \"\\ncounts in ham:\",n_word_ham)\n",
        "\n",
        "p_spam = spams/(hams+spams)  # P(spam)\n",
        "p_ham = hams/(hams+spams)    # P(ham)\n",
        "p_word_given_spam = n_word_spam/spams  # P(word|spam)\n",
        "p_word_given_ham = n_word_ham/hams     # P(word|ham)\n",
        "\n",
        "# p(spam|word) = p(word|spam)*p(spam)/p(word)\n",
        "p_spam_given_word = p_word_given_spam*p_spam/(p_word_given_spam*p_spam + p_word_given_ham*p_ham)\n",
        "# P(ham|word)\n",
        "p_ham_given_word = p_word_given_ham*p_ham/(p_word_given_spam*p_spam + p_word_given_ham*p_ham)\n",
        "print(p_spam_given_word)\n",
        "print(p_ham_given_word)\n",
        "print(\"====================\")\n",
        "print(\"P(ham|free) = \", p_ham_given_word)\n",
        "print(\"spam? \", p_ham_given_word < p_spam_given_word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "counts in spam: 170 \n",
            "counts in ham: 59\n",
            "0.74235807860262\n",
            "0.2576419213973799\n",
            "====================\n",
            "P(ham|free) =  0.2576419213973799\n",
            "spam?  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUzCC6l6knfU"
      },
      "source": [
        "## T3. Spam filter that combines words: Naive Bayes\n",
        "\n",
        "You received a text message \"just do it\" from an unknown sender.\n",
        "\n",
        "Complete the function ***predict()*** that outputs the probability of the message being spam and the predicted label of the message. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_a7X9Mj4CIs"
      },
      "source": [
        "### Questions:\n",
        "\n",
        "*   P ( *spam* | *text* ) = [*3.31e-06*], [5.13e-07 도 맞게 해주세요.]\n",
        "*   Is this text message spam? [*No*]\n",
        "\n",
        "*Note: You do not need to apply a smoothing method here.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDTL_uBGL86f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc107fa1-8e17-4b8c-b5d4-8cd9a9f3736a"
      },
      "source": [
        "text = \"just do it\"\n",
        "\n",
        "# TODOs\n",
        "def predict(text: str):\n",
        "  prob = 1\n",
        "  label = \"spam\"\n",
        "\n",
        "  k = 0.0 # smoothing factor\n",
        "  log_spam = log_ham = 0.0\n",
        "\n",
        "  for token in tokens:\n",
        "    # Calculate p(token|spam), p(token|ham) \n",
        "    word = token\n",
        "    n_word_spam = token_spam_counts[word] # frequency of the word in spam messages\n",
        "    n_word_ham = token_ham_counts[word]   # frequency of the word in ham messages\n",
        "\n",
        "    p_spam = spams/(hams+spams)  # P(spam)\n",
        "    p_ham = hams/(hams+spams)    # P(ham)\n",
        "    p_word_given_spam = (n_word_spam + k) / (spams + 2*k)  # P(word|spam)\n",
        "    p_word_given_ham = (n_word_ham + k) / (hams + 2*k)     # P(word|ham)\n",
        "\n",
        "    # iterating on the bag of words \n",
        "    if token in tokenize(text):\n",
        "      log_spam += math.log(p_word_given_spam)\n",
        "      log_ham += math.log(p_word_given_ham)\n",
        "    else:\n",
        "      log_spam += math.log(1.0 - p_word_given_spam)\n",
        "      log_ham += math.log(1.0 - p_word_given_ham)\n",
        "\n",
        "  p_if_spam = math.exp(log_spam)\n",
        "  p_if_ham = math.exp(log_ham)\n",
        "  prob = p_if_spam / (p_if_spam + p_if_ham)\n",
        "  label = \"spam\" if prob > 0.5 else \"ham\"\n",
        "\n",
        "  return prob, label\n",
        "\n",
        "print(predict(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3.315285589623296e-06, 'ham')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjQ3M91eTG7I"
      },
      "source": [
        "## T4. Smoothing method\n",
        "\n",
        "You again received two text messages from unknown senders.\n",
        "\n",
        "Complete the function ***spamFilter()*** that classifies a given message. \n",
        "\n",
        "You may want to apply a smoothing method for this task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IBahJB5Tlat"
      },
      "source": [
        "### Questions:\n",
        "\n",
        "*   Is textA spam?: [*Yes*]\n",
        "*   Is textB spam?: [*No*]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ66DoXVTpR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1aaf4e4-110d-41ba-d01d-8ba1614c3f25"
      },
      "source": [
        "textA = \"reward! download your free ticket from our website www.pnu.edu\"\n",
        "textB = \"call me and get your money back\"\n",
        "\n",
        "# TODOs\n",
        "def spamFilter(text: str):\n",
        "  k = 1.0 # smoothing factor\n",
        "  log_spam = log_ham = 0.0\n",
        "\n",
        "  for token in tokens:\n",
        "    # Calculate p(token|spam), p(token|ham) \n",
        "    word = token\n",
        "    n_word_spam = token_spam_counts[word] # frequency of the word in spam messages\n",
        "    n_word_ham = token_ham_counts[word]   # frequency of the word in ham messages\n",
        "\n",
        "    p_spam = spams/(hams+spams)  # P(spam)\n",
        "    p_ham = hams/(hams+spams)    # P(ham)\n",
        "    p_word_given_spam = (n_word_spam + k) / (spams + 2*k)  # P(word|spam)\n",
        "    p_word_given_ham = (n_word_ham + k) / (hams + 2*k)     # P(word|ham)\n",
        "\n",
        "    # iterating on the bag of words \n",
        "    if token in tokenize(text):\n",
        "      log_spam += math.log(p_word_given_spam)\n",
        "      log_ham += math.log(p_word_given_ham)\n",
        "    else:\n",
        "      log_spam += math.log(1.0 - p_word_given_spam)\n",
        "      log_ham += math.log(1.0 - p_word_given_ham)\n",
        "  \n",
        "  p_if_spam = math.exp(log_spam)\n",
        "  p_if_ham = math.exp(log_ham)\n",
        "  prob =  p_if_spam / (p_if_spam + p_if_ham)\n",
        "  label = \"spam\" if prob > 0.5 else \"ham\" \n",
        "  return label\n",
        "\n",
        "print(spamFilter(textA))\n",
        "print(spamFilter(textB))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam\n",
            "ham\n"
          ]
        }
      ]
    }
  ]
}
